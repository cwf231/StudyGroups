{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Pipelines: Exercise\n",
    "## Instructions\n",
    "### Objective\n",
    "- Create a machine learning pipeline using the tools you've been introduced to (Pipelines, ColumnTransformer, ML Models).\n",
    "- Fit this Pipeline on a dataset and get predictions.\n",
    "\n",
    "### Requirements\n",
    "- You must perform a **train-test-split** on your data.\n",
    "- Your final pipeline must use the following tools:\n",
    "\n",
    "    1. Simple Imputer\n",
    "    2. Standard Scaler and/or Min-Max Scaler\n",
    "    3. One Hot Encoder\n",
    "    3. Label Encoder\n",
    "    4. Pipeline(s)\n",
    "    5. Column Transformer\n",
    "    6. One of the classifier models you've been introduced to.\n",
    "\n",
    "---\n",
    "\n",
    "# *Tips (if you're stuck)*\n",
    "*<a href='#hints'>Step-by-Step Outline</a> at the bottom of the notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell, but don't edit it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T15:03:02.109197Z",
     "start_time": "2021-03-29T15:03:00.663182Z"
    }
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# This cell defintes the function to create a dataset for us to work with. #\n",
    "############################################################################\n",
    "############################################################################\n",
    "# Run this cell, but skip reading through it for now.                      #\n",
    "############################################################################\n",
    "\n",
    "# Nothing to see here...keep scrolling.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')\n",
    "\n",
    "# I won't comment on this function for the sake of the goal of the exercise.\n",
    "def make_me_a_classification_dataset():\n",
    "    \"\"\"Don't touch this code - that's cheating! :)\"\"\"\n",
    "    \n",
    "    np.random.seed(51)\n",
    "    X, y = make_blobs(\n",
    "        n_samples=10_000,\n",
    "        n_features=12,\n",
    "        cluster_std=2,\n",
    "        centers=3,\n",
    "        random_state=51\n",
    "        )\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        np.concatenate([X, y.reshape(-1, 1)], axis=1),\n",
    "        columns=[f'f{n}' for n in range(1, 13)] + ['target']\n",
    "        )\n",
    "    \n",
    "    ranges_lst = [(np.random.randint(67), np.random.randint(100)) \n",
    "                  for _ in range(9)]\n",
    "    random_cols = np.random.choice(df.columns[:-1], size=9, replace=False)\n",
    "    for rng, col in zip(ranges_lst, random_cols):\n",
    "        m, n = sorted(rng)\n",
    "        scaler = MinMaxScaler(feature_range=(m, n))\n",
    "        new_vals = scaler.fit_transform(df[col].values.reshape(-1, 1))\n",
    "        df[col] = np.round(new_vals, 2)\n",
    "        del scaler\n",
    "        \n",
    "    labels_lst =[\n",
    "        ['very low', 'low', 'high', 'very high'],\n",
    "        ['slow', 'med', 'fast', 'extreme'],\n",
    "        ['low', 'high']\n",
    "    ]\n",
    "    for labels, col in zip(labels_lst, \n",
    "                           [c for c in df.columns if c not in random_cols]):\n",
    "        df[col] = pd.qcut(\n",
    "            df[col], len(labels), labels=labels, duplicates='drop')\n",
    "    \n",
    "    df['target'] = df['target'].map(\n",
    "        {0.0: 'Basset Hound', 1.0: 'Pit-bull', 2.0:'Chihuahua'})\n",
    "    \n",
    "    missing = [\n",
    "        (np.random.randint(1000), \n",
    "         np.random.choice(df.columns[:-1])) \n",
    "        for _ in range(750)\n",
    "    ]\n",
    "    for a, b in missing:\n",
    "        df.loc[a, b] = np.nan\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T15:03:02.113972Z",
     "start_time": "2021-03-29T15:03:02.111595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T15:03:02.554475Z",
     "start_time": "2021-03-29T15:03:02.116384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading data.\n",
    "df = make_me_a_classification_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hints'></a>\n",
    "### *Hints*\n",
    "\n",
    "1. Get a high-level picture of your data.\n",
    "    - *Columns, data-types, missing values, etc.*\n",
    "    \n",
    "    \n",
    "2. Train-Test-Split.\n",
    "\n",
    "\n",
    "3. Create and fit a **Label Encoder** on the `target` (this step is acceptable to perform outside the Pipelines).\n",
    "\n",
    "\n",
    "4. Create Pipelines for each step of your preprocessing.\n",
    "    - *There should be two pipelines: one for each of your numerical and categorical columns.*\n",
    "        - **Imputing** \n",
    "        - **Scaling** \n",
    "        - **One-Hot-Encoding**\n",
    "\n",
    "\n",
    "4. Combine Pipelines in a ColumnTransformer.\n",
    "\n",
    "\n",
    "5. Create a final Pipeline which uses your ColumnTransformer and finishes with a classifier.\n",
    "\n",
    "\n",
    "6. Fit your pipeline on your training data.\n",
    "\n",
    "\n",
    "7. Check your model's performance with at least one metric.\n",
    "    - Compare the model's performance predicting both the `train` and `test` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
