{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3.31\n",
    "# Support Vector Machines\n",
    "\n",
    "## Objectives\n",
    "- <a href='#intro'>Introduction</a> to Support Vector Machines.\n",
    "- Walk through the way SVCs create <a href='#visual'>decision boundaries</a>.\n",
    "- Look at the difference between <a href='#lin_rbf'>Linear and RBF</a> kernels.\n",
    "- Discuss the <a id='#pros_cons'>pros and cons</a> for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "# Introduction\n",
    "> A Support Vector Machine (SVM) is a type of classifier which **modifies the loss function** for optimization to not only take into account overall accuracy metrics of the resulting predictions, but **also to maximize the decision boundary** between the data points. In essence, this further helps tune the classifier as a good balance between underfitting and overfitting.\n",
    "\n",
    "# Visual Walkthrough\n",
    "## Basic Example\n",
    "1. Two Classes\n",
    "<img src='./images/svm1.png' width='300'>\n",
    "\n",
    "2. Maximize the boundary between classes.\n",
    "<img src='./images/svm2.png' width='300'>\n",
    "\n",
    "***Simple Enough!*** *The detailed mathematics can be found in the curriculum.*\n",
    "\n",
    "## C-Value - (***Strength of the magnet***)\n",
    "\n",
    "- **Left:** A *large* value for $C$. \n",
    "    - Misclassifications are heavily punished.\n",
    "        - Optimization prioritizes correct predictions over **size of margin**.\n",
    "        \n",
    "- **Right:** A *small* value for $C$.\n",
    "    - The largest possible margin is achieved, even at the expense of some misclassifications.\n",
    "        - A more generalized model is created.\n",
    "        - Helps with **overfitting**.\n",
    "    \n",
    "<img src='./images/svm3.png' width='600'>\n",
    "\n",
    "- The middle line is the *decision barrier* (hyperplane): \n",
    "    - Where the classification prediction \"flips\" from one to the other.\n",
    "- The outer lines are lines drawn based on the *support vectors*: \n",
    "    - Data points that are closer to the decision barrier and influence the position and orientation of the barrier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visual'></a>\n",
    "# Playing with Decision Boundaries (Hyperplanes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:28.864237Z",
     "start_time": "2021-03-28T15:27:24.308902Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:28.869554Z",
     "start_time": "2021-03-28T15:27:28.866869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting style.\n",
    "plt.style.use(['ggplot', 'seaborn-talk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:29.235982Z",
     "start_time": "2021-03-28T15:27:28.871970Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make some simple data to look at.\n",
    "X, y = make_blobs(\n",
    "    n_features=2, \n",
    "    centers=2, \n",
    "    cluster_std=1.25, \n",
    "    random_state=51)\n",
    "\n",
    "# Transform to df for easy plotting.\n",
    "df = pd.DataFrame(X, columns=['x0', 'x1'])\n",
    "df['y'] = y\n",
    "\n",
    "# Plot data.\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x='x0', y='x1', hue='y', data=df, ax=ax)\n",
    "ax.set(title='Our Data!')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:29.250489Z",
     "start_time": "2021-03-28T15:27:29.238255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train a SVC on the data.\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:29.256783Z",
     "start_time": "2021-03-28T15:27:29.252486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Coordinates of the points that determine the decision boundary.\n",
    "svc.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:29.302675Z",
     "start_time": "2021-03-28T15:27:29.258865Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to find where the predictions change.\n",
    "def get_decision_barrier(X, svc):\n",
    "    \"\"\"Returns the coordinates where the predictions flip.\"\"\"\n",
    "    \n",
    "    X0_vals = np.arange(min(X[:,0].flatten()), max(X[:,0].flatten()), 0.1)\n",
    "    X1_vals = np.arange(min(X[:,1].flatten()), max(X[:,1].flatten()), 0.1)\n",
    "\n",
    "    # Iterate through coordinates to find the decision barrier.\n",
    "    barrier_lst = [] # coordinates where the prediction changes from one to the next.\n",
    "    for x0 in X0_vals:\n",
    "        last_pred = None\n",
    "\n",
    "        for x1 in X1_vals:\n",
    "            curr_pred = svc.predict(np.array([[x0, x1]]))\n",
    "\n",
    "            if last_pred is None: # Checking if this is the first point in the iteration.\n",
    "                last_pred = curr_pred\n",
    "\n",
    "            if curr_pred != last_pred:\n",
    "                barrier_lst.append((x0, x1)) # Append the coordinate where the prediction changes.\n",
    "\n",
    "            last_pred = curr_pred\n",
    "\n",
    "    barrier_lst = np.array(barrier_lst)\n",
    "    return barrier_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:31.197826Z",
     "start_time": "2021-03-28T15:27:29.304781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show the decision boundary.\n",
    "barrier_lst = get_decision_barrier(X, svc)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.scatterplot(x='x0', y='x1', hue='y', data=df, ax=ax)\n",
    "ax.plot(barrier_lst[:,0], barrier_lst[:,1], color='orange', label='Boundary')\n",
    "\n",
    "# Show Support Vectors (points from which the boundaries are drawn).\n",
    "for sv_x, sv_y in svc.support_vectors_:\n",
    "    ax.annotate('SV', xy=(sv_x, sv_y), xytext=(sv_x+1, sv_y+1),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "                )\n",
    "\n",
    "ax.set(title='Our Data\\n& Decision Boundary')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lin_rbf'></a>\n",
    "# Linear vs RBF\n",
    "- There are two main types of SVC kernel - Linear and Radial Basis Function (RBF).\n",
    "    - Linear SVCs use a linear decision boundary to separate classes.\n",
    "    - RBF SVCs use a decision boundary which radiates from certain points in the vector space.\n",
    "\n",
    "\n",
    "> *Visualization Inspiration*: \n",
    "- https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:31.203781Z",
     "start_time": "2021-03-28T15:27:31.201509Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_moons, make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:31.647574Z",
     "start_time": "2021-03-28T15:27:31.205699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make datasets.\n",
    "X, y = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2,\n",
    "    random_state=51, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(51)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [\n",
    "    ('Moons', make_moons(noise=0.1, random_state=51)),\n",
    "    ('Circles', make_circles(noise=0.2, factor=0.5, random_state=51)),\n",
    "    ('Linear', linearly_separable)\n",
    "]\n",
    "\n",
    "# Plot data.\n",
    "fig, ax_lst = plt.subplots(ncols=3, figsize=(12,4))\n",
    "for (title, data), ax in zip(datasets, ax_lst):\n",
    "    X, y = data\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "    ax.set(title=title)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:31.661336Z",
     "start_time": "2021-03-28T15:27:31.649459Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_on_meshgrid(datasets, classifiers, H=0.02, SQ_SIZE=4, plot_sv=True):\n",
    "    \"\"\"\n",
    "    Datasets: list of tuples: (`title`, (X, y))\n",
    "    Classifiers: list of tuples: (`name`, `unfit_clf`)\n",
    "    H: default: 0.02: Step size to take in the meshgrid.\n",
    "    SQ_SIZE: Int: Size of individual subplots.\n",
    "    \n",
    "    If `plot_sv`, the support vectors and decision boundaries will be outlined.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setting subplot specs.\n",
    "    ROWS = len(classifiers)\n",
    "    COLS = len(datasets)\n",
    "    FIGSIZE = (COLS*SQ_SIZE, ROWS*SQ_SIZE)\n",
    "    \n",
    "    # Iterate and plot the model's interaction with the dataset.\n",
    "    fig, ax_lst = plt.subplots(nrows=ROWS, ncols=COLS, figsize=FIGSIZE)\n",
    "    for n_dataset, (data_title, data) in enumerate(datasets):\n",
    "        for n_classifier, (ax, (name, clf)) in \\\n",
    "            enumerate(zip(ax_lst[:, n_dataset], classifiers)):\n",
    "            # Process data.\n",
    "            X, y = data\n",
    "            x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "            y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "            xx, yy = np.meshgrid(np.arange(x_min, x_max, H),\n",
    "                                 np.arange(y_min, y_max, H))\n",
    "\n",
    "            # Fit model.\n",
    "            clf.fit(X, y)\n",
    "\n",
    "            # Plot the decision function (class probabilities).\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            ax.contourf(xx, yy, Z, cmap='viridis', alpha=.8)\n",
    "            \n",
    "            # Plot the decision boundaries and support vectors.\n",
    "            if plot_sv:\n",
    "                ax.contour(\n",
    "                    xx, yy, Z, \n",
    "                    colors=['red', 'blue', 'red'], \n",
    "                    levels=[-1, 0, 1], \n",
    "                    linestyles=[':', '-', ':']\n",
    "                    )\n",
    "\n",
    "            # Plot the data.\n",
    "            ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "\n",
    "            ax.set_xlim(xx.min(), xx.max())\n",
    "            ax.set_ylim(yy.min(), yy.max())\n",
    "            ax.set_xticks(())\n",
    "            ax.set_yticks(())\n",
    "            \n",
    "            # We want to label the rows and columns once only.\n",
    "            # (Leftmost and uppermost)\n",
    "            if n_dataset == 0:\n",
    "                ax.set(ylabel=f'{name} - $C$={clf.C}')\n",
    "            if n_classifier == 0:\n",
    "                ax.set(title=data_title)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:32.882820Z",
     "start_time": "2021-03-28T15:27:31.663155Z"
    }
   },
   "outputs": [],
   "source": [
    "linear_classifiers = [\n",
    "    ('Linear SVC', SVC(kernel=\"linear\", C=0.1)),\n",
    "    ('Linear SVC', SVC(kernel=\"linear\", C=1.0)),\n",
    "    ('Linear SVC', SVC(kernel=\"linear\", C=10.0))\n",
    "]\n",
    "\n",
    "# Plot decsion_function.\n",
    "plot_on_meshgrid(datasets, linear_classifiers, plot_sv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:34.085579Z",
     "start_time": "2021-03-28T15:27:32.884569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot decision function *with support vectors.\n",
    "plot_on_meshgrid(datasets, linear_classifiers, plot_sv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:37.267046Z",
     "start_time": "2021-03-28T15:27:34.087520Z"
    }
   },
   "outputs": [],
   "source": [
    "rbf_classifiers = [\n",
    "    ('RBF SVC', SVC(gamma=2, C=0.1)),\n",
    "    ('RBF SVC', SVC(gamma=2, C=1.0)),\n",
    "    ('RBF SVC', SVC(gamma=2, C=10.0))\n",
    "]\n",
    "\n",
    "# Plot decision function.\n",
    "plot_on_meshgrid(datasets, rbf_classifiers, plot_sv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T15:27:40.577109Z",
     "start_time": "2021-03-28T15:27:37.269478Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add support vectors.\n",
    "plot_on_meshgrid(datasets, rbf_classifiers, plot_sv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pros_cons'></a>\n",
    "# Pros-Cons\n",
    "## Pros\n",
    "- No assumptions!\n",
    "- Good for datasets with high dimensionality.\n",
    "- Very good performance.\n",
    "    - Can approximate complex non-linear functions.\n",
    "\n",
    "## Cons\n",
    "- Long training time.\n",
    "- Lots of tuning required. (`Kernel` & `C` especially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "367.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
