{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3.30\n",
    "# Ensemble Methods\n",
    "## Objectives\n",
    "- Introduce the <a href='#backbone'>backbone of Ensemble methods.</a>\n",
    "- Learn about <a href='#bagging'>Bagging</a> and <a href='#boosting'>Boosting</a> algorithms and some popular models.\n",
    "- <a href='#coding'>Code</a> through an example!\n",
    "\n",
    "\n",
    "- Learn to <a href='#make-your-own'>Make Your Own</a> Ensemble Classifiers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='backbone'></a>\n",
    "# Introduction\n",
    "- An *ensemble* refers to an algorithm that uses more than one model to make a prediction.\n",
    "\n",
    "> **You are looking for investment advice. Instead of asking a single person, you ask three specialists.**\n",
    ">   - **Stock Broker** who is correct 80% of the time.\n",
    ">   - **Finance Professor** who is correct 65% of the time.\n",
    ">   - **Investment Expert** who is correct 85% of the time.\n",
    ">\n",
    "> *If all three experts predict that a given investment is good, what are the odds that all three are wrong?*\n",
    "> \n",
    "> . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If all three experts predict that a given investment is good, \n",
    "# what is the probability that all three are wrong?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bagging'></a>\n",
    "# Bagging\n",
    "\n",
    "*Bootstrap Aggregation*\n",
    "\n",
    "<img src='./images/bagging.png' width='800'>\n",
    "\n",
    "**Training a *bagging classifier*:**\n",
    "- Split training data into a given number of *bags* (with replacement).\n",
    "- Train a classifier on each subset of data.\n",
    "\n",
    "**Predicting with a *bagging classifier*:**\n",
    "- Each classifier makes a prediction.\n",
    "- All predictions are aggregated into a single prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forest\n",
    "- A ***Random Forest*** is an ensemble algorithm which uses $n$-*Decision Trees* as its internal classifiers.\n",
    "- Each *Decision Tree* is trained on **a subset of the data** (both rows *and* features).\n",
    "\n",
    "### Pros and Cons\n",
    "#### Pros\n",
    "- Interpretability.\n",
    "    - Accessible feature importances.\n",
    "- Less data preprocessing required.\n",
    "- Do not overfit (in theory).\n",
    "- Good performance /accuracy.\n",
    "- Robust to noise.\n",
    "\n",
    "#### Cons\n",
    "- Do not predict a continuous output (for regression).\n",
    "- It does not predict beyond the range of the response values in the training data.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "- **n_estimators:**\n",
    "\n",
    "    - It defines the number of decision trees to be created in a random forest.\n",
    "    - Generally, a higher number makes the predictions stronger and more stable, but a very large number can result in higher training time.\n",
    "\n",
    "\n",
    "- **criterion:**\n",
    "\n",
    "    - It defines the function that is to be used for splitting.\n",
    "    - The function measures the quality of a split for each feature and chooses the best split.\n",
    "\n",
    "\n",
    "- **max_features :**\n",
    "\n",
    "    - It defines the maximum number of features allowed for the split in each decision tree.\n",
    "    - Increasing max features usually improve performance but a very high number can decrease the diversity of each tree.\n",
    "\n",
    "\n",
    "- **max_depth:**\n",
    "\n",
    "    - Random forest has multiple decision trees. This parameter defines the maximum depth of the trees.\n",
    "\n",
    "\n",
    "- **min_samples_split:**\n",
    "\n",
    "    - Used to define the minimum number of samples required in a leaf node before a split is attempted.\n",
    "    - If the number of samples is less than the required number, the node is not split.\n",
    "\n",
    "\n",
    "- **min_samples_leaf:** \n",
    "\n",
    "    - This defines the minimum number of samples required to be at a leaf node.\n",
    "    - Smaller leaf size makes the model more prone to capturing noise in train data.\n",
    "\n",
    "\n",
    "- **max_leaf_nodes:** \n",
    "\n",
    "    - This parameter specifies the maximum number of leaf nodes for each tree.\n",
    "    - The tree stops splitting when the number of leaf nodes becomes equal to the max leaf node.\n",
    "\n",
    "---\n",
    "<a id='boosting'></a>\n",
    "# Boosting\n",
    "\n",
    "1. Train a single **weak learner**.\n",
    "    - ***Weak Learner:*** *A simple model that does only slightly better than random guessing.*\n",
    "    \n",
    "    \n",
    "2. Figure out **which examples** the weak learner got wrong.\n",
    "- Build another weak learner that **focuses on the areas the first weak learner got wrong**.\n",
    "- **Continue this process** until a predetermined stopping condition is met, such as until a set number of weak learners have been created, or the model's performance has plateaued.\n",
    "\n",
    "<img src='./images/new_gradient-boosting.png'>\n",
    "\n",
    "- *The weak learners are trained sequentially on the **residuals** of the prior weak learner.*\n",
    "- *Predictions are made where the predictions from each internal classifier are given a **weight of importance**.*\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "### Pros and Cons\n",
    "#### Pros\n",
    "- Doesn't overfit easily.\n",
    "- Few parameters to tune.\n",
    "\n",
    "#### Cons\n",
    "- Can be sensitive to outliers.\n",
    "\n",
    "### Hyperparameters\n",
    "- **base_estimators:** \n",
    "    - It helps to specify the type of base estimator, that is, the machine learning algorithm to be used as base learner.\n",
    "    \n",
    "- **n_estimators:**\n",
    "    - It defines the number of base estimators.\n",
    "    - The default value is 10, but you should keep a higher value to get better performance.\n",
    "    \n",
    "- **learning_rate:** \n",
    "    - This parameter controls the contribution of the estimators in the final combination.\n",
    "    - There is a trade-off between learning_rate and n_estimators.\n",
    "    \n",
    "- **max_depth:**\n",
    "    - Defines the maximum depth of the individual estimator.\n",
    "    - Tune this parameter for best performance.\n",
    "\n",
    "## XGBoost\n",
    "> The boosting algorithm with the highest performance right now is **XGBoost**, which is short for eXtreme Gradient Boosting.\n",
    "> \n",
    "> XGBoost is a stand-alone library that implements popular gradient boosting algorithms in the fastest, most performant way possible. There are many under-the-hood optimizations that allow XGBoost to train more quickly than any other library implementations of gradient boosting algorithms. \n",
    "> For instance, XGBoost is configured in such a way that it parallelizes the construction of trees across all your computer's CPU cores during the training phase. It also allows for more advanced use cases, such as distributing training across a cluster of computers, which is often a technique used to speed up computation. The algorithm even automatically handles missing values!\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "- **nthread**:\n",
    "\n",
    "    - Analogous to learning rate in GBM (*Gradient-Boosted Machine*).\n",
    "    - Makes the model more robust by shrinking the weights on each step.\n",
    "\n",
    "*This is used for parallel processing and the number of cores in the system should be entered..If you wish to run on all cores, do not input this value. The algorithm will detect it automatically.*\n",
    "\n",
    "\n",
    "- **min_child_weight**:\n",
    "\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "\n",
    "\n",
    "- **max_depth**:\n",
    "\n",
    "    - It is used to define the maximum depth.\n",
    "    - Higher depth will allow the model to learn relations very specific to a particular sample.\n",
    "\n",
    "\n",
    "- **max_leaf_nodes**:\n",
    "\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of $n$ would produce a maximum of $2^{n}$ leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "\n",
    "\n",
    "- **gamma**:\n",
    "\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "\n",
    "\n",
    "- **subsample**:\n",
    "\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly sampled for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevent overfitting but values that are too small might lead to under-fitting.\n",
    "\n",
    "\n",
    "- **colsample_bytree**:\n",
    "\n",
    "    - It is similar to max_features in GBM.\n",
    "    - Denotes the fraction of columns to be randomly sampled for each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='coding'></a>\n",
    "# Preparing Some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T13:47:03.808927Z",
     "start_time": "2021-03-23T13:47:02.266184Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-23T13:47:03.836158Z",
     "start_time": "2021-03-23T13:47:03.811383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/diabetes.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='make-your-own'></a>\n",
    "# Make-Your-Own Ensemble\n",
    "## BaggingClassifier\n",
    "- Uses the **Bagging** process with any classifier you choose!\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "    \n",
    "```python\n",
    ">>> bagging = BaggingClassifier(\n",
    "...     KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "```\n",
    "\n",
    "## VotingClassifier\n",
    "- Uses voting / majority-rule for classifiers. \n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier\n",
    "    \n",
    "```python\n",
    ">>> clf1 = LogisticRegression()\n",
    ">>> clf2 = RandomForestClassifier(n_estimators=50)\n",
    ">>> clf3 = GaussianNB()\n",
    "\n",
    ">>> eclf = VotingClassifier(\n",
    "...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)])\n",
    "```\n",
    "\n",
    "## StackingClassifier\n",
    "- Trains a **final estimator** on outputs of the given estimators.\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier\n",
    "    \n",
    "```python\n",
    ">>> clf1 = LogisticRegression()\n",
    ">>> clf2 = RandomForestClassifier(n_estimators=50)\n",
    ">>> clf3 = GaussianNB()\n",
    ">>> clf_final = KNeighborsClassifier()\n",
    "\n",
    ">>> reg = StackingRegressor(\n",
    "...     estimators=[clf1, clf2, clf3],\n",
    "...     final_estimator=clf_final)\n",
    "```\n",
    "\n",
    "# Scikit-Learn Ensembles Documentation \n",
    "- https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
