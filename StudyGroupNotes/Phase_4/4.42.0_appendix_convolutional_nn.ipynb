{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4.42.0 APPENDIX\n",
    "\n",
    "# Convolutional Neural Networks\n",
    "\n",
    "*Reference: <a href='https://github.com/tensorflow/docs/tree/master/site/en/tutorials'>Tensorflow GitHub</a>*\n",
    "\n",
    "## Objectives\n",
    "- Describe the unique types of layers used in Convolutional Nets\n",
    "- Use Tensorflow to build CNNs\n",
    "- Evaluating CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are CNNs?\n",
    "\n",
    "From <a href='https://en.wikipedia.org/wiki/Convolutional_neural_network'>Wikipedia</a>:\n",
    "\n",
    "> \"CNNs are **regularized versions** of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"fully-connectedness\" of these networks makes them prone to **overfitting** data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. However, **CNNs take a different approach towards regularization:** they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns.\"\n",
    ">\n",
    ">\n",
    "> \"Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convoluting and Pooling\n",
    "\n",
    "The two distinctive types of layer inside of a typical CNN (and there may be several of each in a single network) are **convolutional** and **pooling** layers. Let's look at each in turn.\n",
    "\n",
    "\n",
    "### Convolution\n",
    "- Convolutional nets employ <a href='https://en.wikipedia.org/wiki/Convolution'>convolutions</a>, which are a certain kind of transformation. \n",
    "- In the context of neural networks processing images, this can be thought of as sliding a filter (of weights) over the image matrix to produce a new matrix of values.\n",
    "- The relative smallness of the filter means both that there will be relatively few parameters to learn and that the values representing certain areas of the image will be affected only by the values of *nearby areas*. \n",
    "    - This helps the network in **feature detection**. \n",
    "\n",
    "> *Resource & Visuals: https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3x3 image and a 2x2 filter:\n",
    "\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "a & b & c \\\\\n",
    "d & e & g \\\\\n",
    "h & j & k\n",
    "\\end{bmatrix} *\n",
    "\\begin{bmatrix}\n",
    "f_1 & f_2 \\\\\n",
    "f_3 & f_4\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "f_1a + f_2b + f_3d + f_4e & f_1b + f_2c + f_3e + f_4g \\\\\n",
    "f_1d + f_2e + f_3h + f_4j & f_1e + f_2g + f_3j + f_4k\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "\n",
    "- *Line up the filter with the image,*\n",
    "- *multiply all the corresponding pairs,*\n",
    "- *add up those products.* \n",
    "\n",
    "\n",
    "- *Repeat for all positions of the filter as allowed by <a href='https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/'>the stride and the padding</a>. The relative position of the filter to the image will tell you which entry in the resultant matrix you're filling in.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horizontal edge detection filter:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "10 & 10 & 10 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-10 & -10 & -10\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Suppose we apply this filter to (i.e. *convolve*) an image with a clear horizontal edge, such as this one:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "200 & 200 & 200 & 200 & 200 \\\\\n",
    "200 & 200 & 200 & 200 & 200 \\\\\n",
    "200 & 200 & 200 & 200 & 200 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "---\n",
    "\n",
    "Resulting in a *highlighted* horizontal edge:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "6000 & 6000 & 6000 \\\\\n",
    "6000 & 6000 & 6000 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "- The main goal in inserting a *pooling layer* is to reduce dimensionality, which helps to reduce both network computation and model overfitting. \n",
    "- This is generally a matter of reducing a matrix or tensor of values to some smaller size, and the most common way of doing this is by partitioning the large matrix into $n \\times n$ blocks and then replacing each with the largest value in the block. \n",
    "    - Hence: \"*MaxPooling*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [***From TensorFlow***]\n",
    ">\n",
    "> *This tutorial demonstrates training a simple <a href='https://developers.google.com/machine-learning/glossary/#convolutional_neural_network'>Convolutional Neural Network</a> (CNN) to classify MNIST digits. This simple network will achieve over 99% accuracy on the MNIST test set. Because this tutorial uses the <a href='https://www.tensorflow.org/guide/keras'>Keras Sequential API</a>, creating and training our model will take just a few lines of code.*\n",
    ">\n",
    "> *Note: CNNs train faster with a GPU. If you are running this notebook with Colab, you can enable the free GPU via **Edit -> Notebook settings -> Hardware accelerator -> GPU**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Visualization\n",
    "\n",
    "> *https://www.cs.ryerson.ca/~aharley/vis/conv/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:11:55.225873Z",
     "start_time": "2022-01-28T00:11:25.543620Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:11:55.612072Z",
     "start_time": "2022-01-28T00:11:55.229142Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data.\n",
    "((train_images, train_labels), \n",
    "(test_images, test_labels)) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:11:55.623067Z",
     "start_time": "2022-01-28T00:11:55.615051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking shapes.\n",
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:12:53.004032Z",
     "start_time": "2022-01-28T00:12:53.000176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Needing to pass in a 3-dimensional image into CovNet.\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:13:08.060600Z",
     "start_time": "2022-01-28T00:13:07.750701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1.\n",
    "train_images, test_images = train_images / 255, test_images / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Convolutional Base\n",
    "\n",
    "> *The 6 lines of code below define the convolutional base using a common pattern: a stack of <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D'>Conv2D</a> and <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D'>MaxPooling2D</a> layers.*\n",
    "> \n",
    "> - *As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size.*\n",
    ">    - *If you are new to color channels, MNIST has one (because the images are grayscale), whereas a color image has three (R,G,B).* \n",
    "> - *In this example, we will configure our CNN to process inputs of shape (28, 28, 1), which is the format of MNIST images. We do this by passing the argument `input_shape` to our first layer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:14:06.087084Z",
     "start_time": "2022-01-28T00:14:06.082740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:15:47.448015Z",
     "start_time": "2022-01-28T00:15:47.328328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating a model.\n",
    "model = models.Sequential()\n",
    "\n",
    "# Input layer with `input_shape` parameter.\n",
    "model.add(\n",
    "    layers.Conv2D(\n",
    "        32, \n",
    "        (3, 3), \n",
    "        activation='relu', \n",
    "        input_shape=train_images[0].shape,\n",
    "        padding='same')\n",
    "    )\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). \n",
    "- The width and height dimensions tend to shrink as we go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). \n",
    "- Typically,  as the width and height shrink, we can afford (computationally) to add more output channels in each Conv2D layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Dense Layers\n",
    "- To complete our model, we will feed the last output tensor from the convolutional base (of shape (3, 3, 64)) into one or more Dense layers to perform classification. \n",
    "- Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. \n",
    "- First, we will flatten (or unroll) the 3D output to 1D,  then add one or more Dense layers on top. \n",
    "- MNIST has 10 output classes, so we use a final Dense layer with 10 outputs and a softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:18:12.742651Z",
     "start_time": "2022-01-28T00:18:12.712044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 72,842\n",
      "Trainable params: 72,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:18:51.253376Z",
     "start_time": "2022-01-28T00:18:51.226528Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:19:56.123795Z",
     "start_time": "2022-01-28T00:18:55.668357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "938/938 [==============================] - 31s 32ms/step - loss: 0.4589 - accuracy: 0.8645\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 30s 32ms/step - loss: 0.0541 - accuracy: 0.9826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff2b82300b8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:20:31.517935Z",
     "start_time": "2022-01-28T00:20:30.200731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0342 - accuracy: 0.9882\n",
      "Loss\n",
      "\t0.03418765589594841\n",
      "Accuracy\n",
      "\t0.9882000088691711\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(f'Loss\\n\\t{test_loss}\\nAccuracy\\n\\t{test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:21:04.431137Z",
     "start_time": "2022-01-28T00:21:04.096915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show prediction probabilities.\n",
    "plt.bar(\n",
    "    range(10), \n",
    "    (model.predict(test_images[0].reshape(1, 28, 28, 1))).flatten()\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:21:41.905701Z",
     "start_time": "2022-01-28T00:21:41.851525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.4837273e-09, 1.0157312e-08, 2.0635507e-06, 1.7722119e-08,\n",
       "        2.8945049e-11, 6.6747297e-10, 1.2016448e-13, 9.9999738e-01,\n",
       "        1.3660280e-09, 5.9587353e-07]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_images[0].reshape(1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:21:15.954266Z",
     "start_time": "2022-01-28T00:21:15.779490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAANMElEQVR4nO3db4hd9Z3H8c9nY6PBFs2YIQ5pdGIRjC5uUoYYbCguZYN/HsQ8UBqlZFGaPlBpsQ/8sw8aBTEs29Y8WArpJibVrqXQxkSQ2myomIIGR5lqorijcSQJ+XNDwFgRqsl3H8xJd4xzz4z3nPsn+b5fMNx7z/eec74c8sm59/zuvT9HhACc+/6h2w0A6AzCDiRB2IEkCDuQBGEHkjivkzubM2dODA4OdnKXQCpjY2M6duyYJ6tVCrvtGyWtlzRD0n9FxLqy5w8ODmp4eLjKLgGUGBoaalpr+WW87RmS/lPSTZKulrTK9tWtbg9Ae1V5z75E0rsRsS8i/ibpN5JW1NMWgLpVCfs8SfsnPD5QLPsc22tsD9sebjQaFXYHoIq2X42PiA0RMRQRQ/39/e3eHYAmqoT9oKT5Ex5/vVgGoAdVCfurkq60vcD2TEnflbS9nrYA1K3lobeI+Mz2vZJe0PjQ26aI2FtbZwBqVWmcPSKel/R8Tb0AaCM+LgskQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IotKUzbbHJH0k6aSkzyJiqI6mANSvUtgL/xwRx2rYDoA24mU8kETVsIekP9p+zfaayZ5ge43tYdvDjUaj4u4AtKpq2JdFxDcl3STpHtvfPvMJEbEhIoYiYqi/v7/i7gC0qlLYI+JgcXtU0lZJS+poCkD9Wg677Qttf+30fUnLJe2pqzEA9apyNX6upK22T2/nvyPiD7V0BaB2LYc9IvZJ+qcaewHQRgy9AUkQdiAJwg4kQdiBJAg7kEQdX4RJ4ZVXXmlaW79+fem68+bNK63PmjWrtL569erSel9fX0s15MKZHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJx9msrGukdHR9u678cee6y0ftFFFzWtLV26tO52zhqDg4NNaw899FDpupdddlnN3XQfZ3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9ml69tlnm9ZGRkZK173mmmtK63v37i2t7969u7S+bdu2prUXXnihdN0FCxaU1t9///3SehXnnVf+z29gYKC0vn///pb3XTYGL0kPPPBAy9vuVZzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtmnaeHChS3VpuPaa68tra9ataq0vm7duqa1sbGx0nWnGmfft29fab2KmTNnltanGmefqvdGo9G0dtVVV5Wuey6a8sxue5Pto7b3TFjWZ3uH7dHidnZ72wRQ1XRexm+WdOMZyx6UtDMirpS0s3gMoIdNGfaIeEnS8TMWr5C0pbi/RdKt9bYFoG6tXqCbGxGHivuHJc1t9kTba2wP2x4uew8FoL0qX42PiJAUJfUNETEUEUP9/f1VdwegRa2G/YjtAUkqbo/W1xKAdmg17Nslnf5t5dWSmn/HEkBPmHKc3fYzkm6QNMf2AUk/kbRO0m9t3y3pA0m3t7NJlLvgggua1qqOJ1f9DEEVU32P/9ixY6X16667rmlt+fLlLfV0Npsy7BHR7BMd36m5FwBtxMdlgSQIO5AEYQeSIOxAEoQdSIKvuKJrPv7449L6ypUrS+unTp0qrT/xxBNNa7NmzSpd91zEmR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcHV2zefPm0vrhw4dL65dccklp/fLLL/+yLZ3TOLMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMs6Ot3nvvvaa1+++/v9K2X3755dL6pZdeWmn75xrO7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsaKvnnnuuae3TTz8tXfe2224rrV9xxRUt9ZTVlGd225tsH7W9Z8KytbYP2h4p/m5ub5sAqprOy/jNkm6cZPnPI2JR8fd8vW0BqNuUYY+IlyQd70AvANqoygW6e22/UbzMn93sSbbX2B62PdxoNCrsDkAVrYb9F5K+IWmRpEOSftrsiRGxISKGImKov7+/xd0BqKqlsEfEkYg4GRGnJP1S0pJ62wJQt5bCbntgwsOVkvY0ey6A3jDlOLvtZyTdIGmO7QOSfiLpBtuLJIWkMUk/aF+L6GVTjZVv3bq1ae38888vXffxxx8vrc+YMaO0js+bMuwRsWqSxRvb0AuANuLjskAShB1IgrADSRB2IAnCDiTBV1xRycaN5QMzu3btalq74447StflK6z14swOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzo5SIyMjpfX77ruvtH7xxRc3rT366KMtdIRWcWYHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZ0/uk08+Ka2vWjXZjwv/v5MnT5bW77zzzqY1vq/eWZzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtnPcadOnSqt33LLLaX1d955p7S+cOHC0vojjzxSWkfnTHlmtz3f9p9sv2V7r+0fFsv7bO+wPVrczm5/uwBaNZ2X8Z9J+nFEXC1pqaR7bF8t6UFJOyPiSkk7i8cAetSUYY+IQxHxenH/I0lvS5onaYWkLcXTtki6tU09AqjBl7pAZ3tQ0mJJuyXNjYhDRemwpLlN1llje9j2cKPRqNIrgAqmHXbbX5X0O0k/iogTE2sREZJisvUiYkNEDEXEUH9/f6VmAbRuWmG3/RWNB/3XEfH7YvER2wNFfUDS0fa0CKAOUw692bakjZLejoifTShtl7Ra0rridltbOkQlx48fL62/+OKLlbb/1FNPldb7+voqbR/1mc44+7ckfU/Sm7ZHimUPazzkv7V9t6QPJN3elg4B1GLKsEfEnyW5Sfk79bYDoF34uCyQBGEHkiDsQBKEHUiCsANJ8BXXc8CHH37YtLZ06dJK23766adL64sXL660fXQOZ3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9nPAk08+2bS2b9++SttetmxZaX385w5wNuDMDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM5+FhgdHS2tr127tjON4KzGmR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkpjO/OzzJf1K0lxJIWlDRKy3vVbS9yU1iqc+HBHPt6vRzHbt2lVaP3HiRMvbXrhwYWl91qxZLW8bvWU6H6r5TNKPI+J121+T9JrtHUXt5xHxH+1rD0BdpjM/+yFJh4r7H9l+W9K8djcGoF5f6j277UFJiyXtLhbda/sN25tsz26yzhrbw7aHG43GZE8B0AHTDrvtr0r6naQfRcQJSb+Q9A1JizR+5v/pZOtFxIaIGIqIof7+/uodA2jJtMJu+ysaD/qvI+L3khQRRyLiZESckvRLSUva1yaAqqYMu8d/PnSjpLcj4mcTlg9MeNpKSXvqbw9AXaZzNf5bkr4n6U3bI8WyhyWtsr1I48NxY5J+0Ib+UNH1119fWt+xY0dpnaG3c8d0rsb/WdJkPw7OmDpwFuETdEAShB1IgrADSRB2IAnCDiRB2IEk+Cnps8Bdd91VqQ5InNmBNAg7kARhB5Ig7EAShB1IgrADSRB2IAlHROd2ZjckfTBh0RxJxzrWwJfTq731al8SvbWqzt4uj4hJf/+to2H/ws7t4YgY6loDJXq1t17tS6K3VnWqN17GA0kQdiCJbod9Q5f3X6ZXe+vVviR6a1VHeuvqe3YAndPtMzuADiHsQBJdCbvtG22/Y/td2w92o4dmbI/ZftP2iO3hLveyyfZR23smLOuzvcP2aHE76Rx7Xeptre2DxbEbsX1zl3qbb/tPtt+yvdf2D4vlXT12JX115Lh1/D277RmS/lfSv0g6IOlVSasi4q2ONtKE7TFJQxHR9Q9g2P62pL9K+lVE/GOx7N8lHY+IdcV/lLMj4oEe6W2tpL92exrvYraigYnTjEu6VdK/qovHrqSv29WB49aNM/sSSe9GxL6I+Juk30ha0YU+el5EvCTp+BmLV0jaUtzfovF/LB3XpLeeEBGHIuL14v5Hkk5PM97VY1fSV0d0I+zzJO2f8PiAemu+95D0R9uv2V7T7WYmMTciDhX3D0ua281mJjHlNN6ddMY04z1z7FqZ/rwqLtB90bKI+KakmyTdU7xc7Ukx/h6sl8ZOpzWNd6dMMs3433Xz2LU6/XlV3Qj7QUnzJzz+erGsJ0TEweL2qKSt6r2pqI+cnkG3uD3a5X7+rpem8Z5smnH1wLHr5vTn3Qj7q5KutL3A9kxJ35W0vQt9fIHtC4sLJ7J9oaTl6r2pqLdLWl3cXy1pWxd7+Zxemca72TTj6vKx6/r05xHR8T9JN2v8ivx7kv6tGz006esKSX8p/vZ2uzdJz2j8Zd2nGr+2cbekSyTtlDQq6X8k9fVQb09JelPSGxoP1kCXelum8Zfob0gaKf5u7vaxK+mrI8eNj8sCSXCBDkiCsANJEHYgCcIOJEHYgSQIO5AEYQeS+D9ba+dQO9QYHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show actual image.\n",
    "plt.imshow(test_images[0], cmap='Greys');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Image Processing / Augmentation\n",
    "\n",
    "The `ImageDataGenerator` is commonly used to bolster and enhance your training images by rotating and shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:26:09.101657Z",
     "start_time": "2022-01-28T00:26:09.098278Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:26:10.802398Z",
     "start_time": "2022-01-28T00:26:10.797402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.preprocessing.image.ImageDataGenerator at 0x7ff2b8a5a8d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Generate batches of tensor image data with real-time data augmentation.\n",
    "\n",
    " The data will be looped over (in batches).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:30:14.755797Z",
     "start_time": "2022-01-28T00:30:14.752657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create data generator.\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T00:30:15.114475Z",
     "start_time": "2022-01-28T00:30:15.109767Z"
    }
   },
   "outputs": [],
   "source": [
    "# datagen.fit(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example:*\n",
    "<img src='./images/cat_data_augmentation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "#### Flow / Flow from DataFrame\n",
    "- If the data is loaded into memory, you can use `.flow()` or `.flow_from_dataframe()` to access it:\n",
    "\n",
    "> ```python\n",
    "> train_generator = datagen.flow(train_images, train_labels, batch_size=32)\n",
    "> \n",
    "> model.fit(\n",
    ">     train_generator,\n",
    ">     steps_per_epoch=train_images.shape[0] // 32, \n",
    ">     epochs=10\n",
    ">     )\n",
    "> ```\n",
    "#### Flow from Directory\n",
    "- If the data is in a directory and not loaded into memory, you can use `.flow_from_directory()` to access it:\n",
    "\n",
    "> ```\n",
    "> train_generator = train_datagen.flow_from_directory('data/train')\n",
    "> validation_generator = test_datagen.flow_from_directory('data/validation')\n",
    "> \n",
    "> model.fit(\n",
    ">         train_generator,\n",
    ">         epochs=50,\n",
    ">         validation_data=validation_generator,\n",
    ">         validation_steps=800)\n",
    "> ```\n",
    "\n",
    "> **directory:**\n",
    "> \n",
    "> **string, path to the target directory. It should contain one subdirectory per class. Any PNG, JPG, BMP, PPM or TIF images inside each of the subdirectories directory tree will be included in the generator.**\n",
    "\n",
    "In summary, this is our directory structure:\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "323px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
