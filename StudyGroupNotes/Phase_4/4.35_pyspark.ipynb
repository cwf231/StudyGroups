{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Phase 4.35\n",
    "\n",
    "# Pyspark\n",
    "\n",
    "## Objectives\n",
    "- Get an introduction to <a href='#big_data'>Big Data</a>.\n",
    "    - Discuss <a href='#parallel'>Parallel and Distributed Computing</a>.\n",
    "    - Look at the <a href='#mapreduce'>MapReduce</a> process.\n",
    "- Get started with <a href='#start'>Docker and PySpark</a>.\n",
    "    - Discuss <a href='#rdd'>RDDs and DataFrames</a>.\n",
    "- Code through an example in PySpark in <a href='#colab'>Google Colab</a>.\n",
    "\n",
    "<a id='big_data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Big Data\n",
    "## What is *Big Data*\n",
    ">***Big data is high-volume, high-velocity and/or high-variety information assets that demand cost-effective, innovative forms of information processing that enable enhanced insight, decision making, and process automation***.\n",
    "> *https://www.gartner.com/en/information-technology/glossary/big-data*\n",
    "\n",
    "<img src='./images/3_components.png' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Velocity\n",
    "<img src='./images/velo.jpeg' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variety\n",
    "<img src='./images/variety.png' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Big Data Analytics\n",
    "The key activities associated with big data analytics are reflected in four main areas: \n",
    "\n",
    "- Warehousing and distribution.\n",
    "- Storage.\n",
    "- Computational platforms.\n",
    "- Analyses, visualization, and evaluation.\n",
    "\n",
    "<img src='./images/tech_stack.png'>\n",
    "\n",
    "*Such a framework can be applied for knowledge discovery and informed decision-making in big data-driven organizations.*\n",
    "\n",
    "<a id='parallel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel & Distributed Computing\n",
    "- **MapReduce** is a programming paradigm that enables the ability to scale across hundreds or thousands of servers for big data analytics. \n",
    "\n",
    "\n",
    "> The term \"MapReduce\" refers to two distinct tasks. \n",
    "> \n",
    "> - The first is the **Map** job, which takes one set of data and transforms it into another set of data, where individual elements are broken down into tuples **(key/value pairs)**, \n",
    ">\n",
    "> - The **Reduce** job takes the output from a map as input and combines those data tuples into a smaller set of tuples.\n",
    "\n",
    "\n",
    "- The MapReduce programming paradigm is designed to allow **parallel and distributed processing**  of large sets of data (also known as big data). MapReduce allows us to convert such big datasets into sets of **tuples** as **key: value** pairs,\n",
    "\n",
    "\n",
    "- Somehow, all data can be mapped to **key: value** pairs.\n",
    "- Keys and values themselves can be of ANY data type.\n",
    "\n",
    "\n",
    "> In simpler terms, **MapReduce uses parallel distributed computing to turn big data into regular data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distributed Processing\n",
    "\n",
    "- *A distributed processing system is a group of computers in a network working in tandem to accomplish a task.*\n",
    "<img src='./images/types_of_network.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parallel Processing Systems\n",
    "\n",
    "With parallel computing:\n",
    "1. A larger problem is broken up into smaller pieces.\n",
    "2. Every part of the problem follows a series of instructions.\n",
    "3. Each one of the instructions is executed simultaneously on different processors.\n",
    "4. All of the answers are collected from the small problems and combined into one final answer.\n",
    "\n",
    "---\n",
    "\n",
    "In the image below, you can see a simple example of a process being broken up and completed both sequentially and in parallel.\n",
    "\n",
    "<img src='./images/parallel.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='mapreduce'></a>\n",
    "### MapReduce  Example\n",
    "Here are the first five zoos the data scientist reads over in the data document they receive:\n",
    "\n",
    "| Animals              |\n",
    "|----------------------|\n",
    "| lion tiger bear      |\n",
    "| lion giraffe         |\n",
    "| giraffe penguin      |\n",
    "| penguin lion giraffe |\n",
    "| koala giraffe        |\n",
    "\n",
    "\n",
    "Let's now look at how you would use the MapReduce framework in this simple word count example that could be generalized to much more data.\n",
    "\n",
    "<img src=\"./images/word_count.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. MAP Task (Splitting & Mapping)\n",
    "- Data transformed into **key:value** pairs and split into fragments, which are then assigned to map tasks. \n",
    "    - Each computing cluster is assigned a number of map tasks, which are subsequently distributed among its nodes.\n",
    "\n",
    "- We will then use the map function to create key:value pairs represented by:   \n",
    "> `{animal}: {# of animals per zoo}`\n",
    "\n",
    "- After processing of the original key:value pairs, some __intermediate__ key:value pairs are generated. \n",
    "    - The intermediate key:value pairs are __sorted by their key values__ to create a new list of key:value pairs.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2. Shuffling\n",
    "- This list from the map task is divided into a new set of fragments\n",
    "    - That sorts and shuffles the mapped objects into an order or grouping that will make it easier to reduce them. \n",
    "\n",
    "- **The number of these new fragments will be the same as the number of the reduce tasks**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. REDUCE Task (Reducing)\n",
    "\n",
    "- Now, every properly shuffled segment will have a reduce task applied to it. \n",
    "\n",
    "    - After the task is completed, the final output is written onto a file system. \n",
    "    - The underlying file system is usually HDFS (Hadoop Distributed File System). \n",
    "    \n",
    "    \n",
    "- It's important to note that MapReduce will generally only be powerful when dealing with large amounts of data. \n",
    "    - When working with a small dataset, it will be faster not to perform operations in the MapReduce framework.\n",
    "\n",
    "\n",
    "- There are two groups of entities in this process to ensuring that the MapReduce task gets done properly:\n",
    "\n",
    "    1. **Job Tracker**: a \"master\" node that informs the other nodes which map and reduce jobs to complete.\n",
    "\n",
    "    2. **Task Tracker**: the \"worker\" nodes that complete the map and reduce operations.\n",
    "\n",
    "There are different names for these components depending on the technology used, but there will always be a master node that informs worker nodes what tasks to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Spark\n",
    "<a id='start'></a>\n",
    "## Getting Started with PySpark and Docker\n",
    "### Docker installation directions below:\n",
    "\n",
    "1. **Install Docker Desktop. (or Docker Toolbox if you have Windows 10 Home).**\n",
    "\n",
    "\n",
    "2. **Pull the pyspark-notebook image (this can take up to 20 minutes!)**\n",
    "\n",
    "```bash\n",
    "docker pull jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "3. **I recommend creating a new folder for all of the pyspark-related labs. (I called mine Docker).**\n",
    "    - Navigate to this folder containing the cloned repositories (NOT the folder for an individual repo).\n",
    "    - Whatever folder you are in when you run this command will show up inside jupyter.\n",
    "\n",
    "\n",
    "4A. **Start the container with port forwarding**\n",
    "```bash\n",
    "docker run -it --name my-pyspark1 -p 8888:8888 -v \"${PWD}:/home/jovyan/work\" jupyter/pyspark-notebook \n",
    "```\n",
    "\n",
    "4B. **NOTE: If you have an issue with the ports clashing with your local jupyter notebook server:**\n",
    ">- Change the port numbers from 8888 to something elise (e.g. 8989)\n",
    ">- Add the `jupyter notebook` launch command to the end with `--no-browser --ip=0.0.0.0 --port=8989` (changing 899 to whatever port number you used).\n",
    "\n",
    "- The full command would be:\n",
    "```bash\n",
    "docker run -it --name my-pyspark1 -p 8989:8989 -v \"${PWD}:/home/jovyan/work\" jupyter/pyspark-notebook jupyter notebook --no-browser --ip=0.0.0.0 --port=8989\n",
    "```\n",
    "\n",
    "5. **Copy and paste the url thats starts with 127.0.0.1 displayed in the terminal into your web browser.**\n",
    "    - NOTE to Windows users using Docker Toolbox:\n",
    "        - change the ip address of the url displayed from `127.0.0.1` to \n",
    "\n",
    "6. To stop the container, in you terminal hit Control+C\n",
    "\n",
    "7. **To resume the container:**  (note: any pip installs or settings will be saved if you resume a stopped container)\n",
    "\n",
    "```bash\n",
    "docker start -ia my-pyspark1\n",
    "```\n",
    "\n",
    "8. To remove it:\n",
    "\n",
    "```bash\n",
    "docker rm my-pyspark1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='rdd'></a>\n",
    "## RDD & DataFrames\n",
    "\n",
    "Resilient Distributed Datasets\n",
    "\n",
    "<img src='./images/DataFrames.png'>\n",
    "\n",
    "> *(From DataBricks)*\n",
    ">\n",
    "> *https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html*\n",
    ">\n",
    "> ### Use RDDs when:\n",
    "> - You want low-level transformation and actions and control on your dataset;\n",
    "> - Your data is unstructured, such as media streams or streams of text;\n",
    "> - You want to manipulate your data with functional programming constructs than domain specific expressions;\n",
    "> - You don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column; and\n",
    "> - You can forgo some optimization and performance benefits available with DataFrames and Datasets for structured and semi-structured data.\n",
    ">\n",
    ">\n",
    "> ### Using Datasets / DataFrames:\n",
    "> - You want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame or Dataset.\n",
    "> - Your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame or Dataset.\n",
    "> - You want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset.\n",
    "> - You want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\n",
    "> - You are a R user, use DataFrames.\n",
    "> - You are a Python user, use DataFrames and resort back to RDDs if you need more control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Additional Resources\n",
    "\n",
    "- <a href='https://medium.datadriveninvestor.com/distributed-data-processing-with-apache-spark-2a5e473b0cb1'>Medium - Distributed Data Processing with Apache Spark</a>\n",
    "    - A very thorough talk through Spark. Very informative.\n",
    "    \n",
    "    \n",
    "- <a href='https://github.com/apache/spark/blob/v3.1.1-rc3/examples/src/main/python/ml/logistic_regression_with_elastic_net.py'>Official Example - Logistic Regression with PySpark ML Library</a> \n",
    "    - From the Apache github. \n",
    "    - Uses `pyspark.ml.classification.LogisticRegression` \n",
    "    \n",
    "    \n",
    "- <a href='http://spark.apache.org/docs/latest/api/python/index.html'>PySpark Documentation</a>\n",
    "    - Official Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='colab'></a>\n",
    "# *Colab Notebook*\n",
    "***Demo Notebook - Google Colab***\n",
    "> *https://colab.research.google.com/drive/1EO_qvNC6BAx31RaAMlPjYI6Ftj1yjIFH?usp=sharing*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "343px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
